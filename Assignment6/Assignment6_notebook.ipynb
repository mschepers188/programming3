{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60410a58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12530/1331118966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFloatType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8cb86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 19 kB/s s eta 0:00:01    |██████████▊                     | 94.1 MB 10.7 MB/s eta 0:00:18     |████████████                    | 106.4 MB 13.2 MB/s eta 0:00:14     |██████████████████████▌         | 198.0 MB 12.2 MB/s eta 0:00:07     |██████████████████████▉         | 200.5 MB 12.1 MB/s eta 0:00:07     |███████████████████████▍        | 206.0 MB 12.1 MB/s eta 0:00:07\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764027 sha256=bcf53137781559251c88ec47a0cbfde08ba331fcef353ead4ad70c88ab887020\n",
      "  Stored in directory: /home/martin/.cache/pip/wheels/1d/27/68/1382001655ef41217e1dd34d59aa777612135379bab64279e9\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSTool:\n",
    "    def __init__(self):\n",
    "        print('Creating output folder')\n",
    "        if os.path.exists('output'):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs('output')\n",
    "\n",
    "    def pyspark_session(self, host_location):\n",
    "        \"\"\"\n",
    "        Creates and returns spark session object\n",
    "        \"\"\"\n",
    "        print('Starting session')\n",
    "        sc = SparkContext(host_location)  # Create spark context\n",
    "        spark = SparkSession(sc)  # Create session\n",
    "        return spark\n",
    "\n",
    "    def file_loader(self, path, delim, spark_obj):\n",
    "        print('Loading in file')\n",
    "        data = spark_obj.read.options(delimiter=delim).csv(path)  # load file\n",
    "        print('File loaded')\n",
    "        return data\n",
    "\n",
    "    def get_questions(self, df):\n",
    "    \tpass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pstool = PSTool()  # Instanciate object\n",
    "    spk = pstool.pyspark_session('local[16]')  # start session\n",
    "    # load data\n",
    "    path = '/data/dataprocessing/interproscan/all_bacilli.tsv'\n",
    "    # path = 'all_bacilli_subset.tsv'\n",
    "    df = pstool.file_loader(path, '\\t', spk)\n",
    "    pstool.get_questions(df)\n",
    "    print('Closing spark session')\n",
    "    spk.sparkContext.stop()\n",
    "    # df.printSchema()  # Shows column names and some info\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
