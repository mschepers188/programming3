{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60410a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,FloatType\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d01b426",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "* https://www.hackdeploy.com/pyspark-one-hot-encoding-with-countvectorizer/\n",
    "* https://techinplanet.com/split-lists-in-a-column-into-one-hot-encoded-features-in-pyspark/\n",
    "* https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html\n",
    "* https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4a06db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output folder\n",
      "Starting session\n",
      "Loading in file\n",
      "File loaded\n"
     ]
    }
   ],
   "source": [
    "class PSTool:\n",
    "    def __init__(self):\n",
    "        print('Creating output folder')\n",
    "        if os.path.exists('output'):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs('output')\n",
    "\n",
    "    def pyspark_session(self, host_location):\n",
    "        \"\"\"\n",
    "        Creates and returns spark session object\n",
    "        \"\"\"\n",
    "        print('Starting session')\n",
    "        sc = SparkContext(host_location)  # Create spark context\n",
    "        spark = SparkSession(sc)  # Create session\n",
    "        return spark, sc\n",
    "\n",
    "    def file_loader(self, path, delim, spark_obj, schema):\n",
    "        print('Loading in file')\n",
    "        data = spark_obj.read.options(delimiter=delim).option(\"header\",\"False\").csv(path, schema=schema)\n",
    "        \n",
    "        print('File loaded')\n",
    "        return data\n",
    "\n",
    "    def get_questions(self, df):\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pstool = PSTool()  # Instanciate object\n",
    "    spk, sc = pstool.pyspark_session('local[16]')  # start session\n",
    "    # load data\n",
    "#     path = '/data/dataprocessing/interproscan/all_bacilli.tsv'\n",
    "    path = 'all_bacilli_subset.tsv'\n",
    "    schema = StructType([\n",
    "        StructField(\"Protein_accession\", StringType(), True),\n",
    "        StructField(\"Sequence_MD5_digest\", StringType(), True),\n",
    "        StructField(\"Sequence_length\", IntegerType(), True),\n",
    "        StructField(\"Analysis\", StringType(), True),\n",
    "        StructField(\"Signature_accession\", StringType(), True),\n",
    "        StructField(\"Signature_description\", StringType(), True),\n",
    "        StructField(\"Start_location\", IntegerType(), True),\n",
    "        StructField(\"Stop_location\", IntegerType(), True),\n",
    "        StructField(\"Score\", FloatType(), True),\n",
    "        StructField(\"Status\", StringType(), True),\n",
    "        StructField(\"Date\", StringType(), True),\n",
    "        StructField(\"InterPro_annotations_accession\", StringType(), True),\n",
    "        StructField(\"InterPro_annotations_description\", StringType(), True),\n",
    "        StructField(\"GO_annotations\", StringType(), True),\n",
    "        StructField(\"Pathways_annotations\", StringType(), True)])\n",
    "    \n",
    "    df = pstool.file_loader(path, '\\t', spk, schema)\n",
    "#     pstool.get_questions(df)\n",
    "#     print('Closing spark session')\n",
    "#     spk.sparkContext.stop()\n",
    "#     df.printSchema()  # Shows column names and some info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d392b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 15\n",
      "count: 200\n"
     ]
    }
   ],
   "source": [
    "print('len:', len(df.columns))\n",
    "print('count:' , df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a51d1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Protein_accession',\n",
       " 'Sequence_MD5_digest',\n",
       " 'Sequence_length',\n",
       " 'Analysis',\n",
       " 'Signature_accession',\n",
       " 'Signature_description',\n",
       " 'Start_location',\n",
       " 'Stop_location',\n",
       " 'Score',\n",
       " 'Status',\n",
       " 'Date',\n",
       " 'InterPro_annotations_accession',\n",
       " 'InterPro_annotations_description',\n",
       " 'GO_annotations',\n",
       " 'Pathways_annotations']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc446aae",
   "metadata": {},
   "source": [
    "# Data munging\n",
    "\n",
    "First, the data has to be munged. \n",
    "* Remove entries without InterPRO number\n",
    "* Note which proteins features are >90% size of the protein and remove them. \"ProtID_select\"\n",
    "* Select the smaller features for the above found proteins. Proteins without large feature do not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375c16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPRO_filt = df.filter(df[\"InterPro_annotations_accession\"] != '-').filter(df['Signature_description'] != '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dde0d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 15\n",
      "count: 97\n"
     ]
    }
   ],
   "source": [
    "print('len:', len(IPRO_filt.columns))\n",
    "print('count:' , IPRO_filt.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15359443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Protein_accession: string, Sequence_MD5_digest: string, Sequence_length: int, Analysis: string, Signature_accession: string, Signature_description: string, Start_location: int, Stop_location: int, Score: float, Status: string, Date: string, InterPro_annotations_accession: string, InterPro_annotations_description: string, GO_annotations: string, Pathways_annotations: string, perc: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sizes = IPRO_filt.withColumn('perc', abs(df.Start_location - df.Stop_location) / df.Sequence_length).sort('perc')\n",
    "df_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3abbac3",
   "metadata": {},
   "source": [
    "## Get descriptions into arrays\n",
    "\n",
    "This will be done to be able to one-hot encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78fadca2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         nameAsArray|\n",
      "+--------------------+\n",
      "|[Pyruvate, kinase...|\n",
      "|[Pyruvate, kinase...|\n",
      "|[Bacterial, senso...|\n",
      "|[Pyruvate, kinase...|\n",
      "|[Pyruvate, kinase...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_array = df_sign.select(split(df.Signature_description,\" \").alias(\"nameAsArray\"))\n",
    "df_array.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1552a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "df_copy = df_array.select('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed6434d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPRO_filt.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54508e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concat = df_copy.join(IPRO_filt.select('Sequence_length', 'Start_location', 'Stop_location', 'Score'))\n",
    "# df_concat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2032dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get term frequency vector through HashingTF\n",
    "from pyspark.ml.feature import HashingTF\n",
    "ht = HashingTF(inputCol=\"nameAsArray\", outputCol=\"features\")\n",
    "result = ht.transform(df_copy)\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94c0d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         nameAsArray|            features|\n",
      "+--------------------+--------------------+\n",
      "|[Pyruvate, kinase...|(262144,[25702,61...|\n",
      "|[Pyruvate, kinase...|(262144,[115560,2...|\n",
      "|[Bacterial, senso...|(262144,[41004,41...|\n",
      "|[Pyruvate, kinase...|(262144,[115560,2...|\n",
      "|[Pyruvate, kinase...|(262144,[115560,2...|\n",
      "|[Pyruvate, kinase...|(262144,[115560,2...|\n",
      "|[Bacterial, senso...|(262144,[41004,41...|\n",
      "|[Bacterial, senso...|(262144,[41004,41...|\n",
      "|[Pyruvate, kinase...|(262144,[115560,2...|\n",
      "|[Pyruvate, kinase...|(262144,[115560,2...|\n",
      "|[Bacterial, senso...|(262144,[41004,41...|\n",
      "|[Pyruvate, kinase...|(262144,[115560,2...|\n",
      "|[PpiC-type, pepti...|(262144,[7386,618...|\n",
      "|[Aminotransferase...|(262144,[19690,14...|\n",
      "|[Transaminase_4ab...|(262144,[99950],[...|\n",
      "|[PEP-utilising, e...|(262144,[72259,10...|\n",
      "|[Nudix, box, sign...|(262144,[22370,61...|\n",
      "|[PK, beta-barrel,...|(262144,[33826,16...|\n",
      "|       [GAF, domain]|(262144,[78676,10...|\n",
      "|[Ribosomal, prote...|(262144,[41365,61...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d63f6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(IPRO_filt.select(['Sequence_length', 'Start_location', 'Stop_location', 'Score']).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bebb5aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 1, 262144)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_3d = np.array(result.select('features').collect())\n",
    "x_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81a3e9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_3d[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c751af91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 262144)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped = x_3d.reshape(97, -1)\n",
    "reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4304b644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ca215a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = np.concatenate([test, reshaped], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0948e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[547, 2, 131, ..., 0.0, 0.0, 0.0],\n",
       "       [547, 161, 547, ..., 0.0, 0.0, 0.0],\n",
       "       [547, 159, 547, ..., 0.0, 0.0, 0.0],\n",
       "       ...,\n",
       "       [589, 258, 361, ..., 0.0, 0.0, 0.0],\n",
       "       [589, 430, 574, ..., 0.0, 0.0, 0.0],\n",
       "       [589, 468, 577, ..., 0.0, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82ac8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_from_df(data, column_list):\n",
    "    return np.array(data.select(column_list).collect())\n",
    "    \n",
    "labels = get_array_from_df(IPRO_filt, 'InterPro_annotations_accession')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e67877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_range = []\n",
    "for i in range(numeric_data.shape[1]):\n",
    "    col_range.append(f'Col_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "911c1ebe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[16]) created by __init__ at C:\\Users\\MARTIN~1\\AppData\\Local\\Temp/ipykernel_5172/3308608308.py:14 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MARTIN~1\\AppData\\Local\\Temp/ipykernel_5172/62283380.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local[16]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumeric_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    193\u001b[0m             )\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                     \u001b[1;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    431\u001b[0m                         \u001b[1;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m                         \u001b[1;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[16]) created by __init__ at C:\\Users\\MARTIN~1\\AppData\\Local\\Temp/ipykernel_5172/3308608308.py:14 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext('local[16]')\n",
    "df_final = spk.parallelize(numeric_data).map(lambda x: [float(i) for i in x]).toDF(col_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a41d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', \n",
    "                         labelCol='churnIn3Month')\n",
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee463028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
