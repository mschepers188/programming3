{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f95288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/commons/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/06/17 14:56:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "\n",
    "class PSTool:\n",
    "    def pyspark_session(self, host_location):\n",
    "        \"\"\"\n",
    "        Creates and returns spark session object\n",
    "        \"\"\"\n",
    "        sc = SparkContext(host_location)  # Create spark context\n",
    "        spark = SparkSession(sc)  # Create session\n",
    "        return spark\n",
    "\n",
    "    def file_loader(self, path, delim, spark_obj):\n",
    "        data = spark_obj.read.options(delimiter=delim).csv(path)  # load file\n",
    "        return data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Instanciate object\n",
    "    pstool = PSTool()\n",
    "    # start session\n",
    "    spk = pstool.pyspark_session('local')\n",
    "    # load data\n",
    "    path = 'all_bacilli_subset.tsv'\n",
    "    df=pstool.file_loader(path, '\\t', spk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24728a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b512cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Protein _accession', 'Sequence_MD5_digest', 'Sequence_length', 'Analysis',\n",
    "# 'Signature_accession', 'Signature_description', 'Start location', 'Stop location',\n",
    "# 'Score', 'Status', 'Date', 'InterPro annotations', 'InterPro annotations' - description (e.g. BRCA2 repeat)\n",
    "#     (GO annotations (e.g. GO:0005515) - optional column; only displayed if –goterms option is switched on)\n",
    "#     (Pathways annotations (e.g. REACT_71) - optional column; only displayed if –pathways option is switched on)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0aa1ac",
   "metadata": {},
   "source": [
    "The TSV format presents the match data in columns as follows:\n",
    "\n",
    "    1. Protein accession (e.g. P51587)\n",
    "    2. Sequence MD5 digest (e.g. 14086411a2cdf1c4cba63020e1622579)\n",
    "    3. Sequence length (e.g. 3418)\n",
    "    4. Analysis (e.g. Pfam / PRINTS / Gene3D)\n",
    "    5. Signature accession (e.g. PF09103 / G3DSA:2.40.50.140)\n",
    "    6. Signature description (e.g. BRCA2 repeat profile)\n",
    "    7. Start location\n",
    "    8. Stop location\n",
    "    9. Score - is the e-value (or score) of the match reported by member database method (e.g. 3.1E-52)\n",
    "    10. Status - is the status of the match (T: true)\n",
    "    11. Date - is the date of the run\n",
    "    12. InterPro annotations - accession (e.g. IPR002093)\n",
    "    13. InterPro annotations - description (e.g. BRCA2 repeat)\n",
    "    14. (GO annotations (e.g. GO:0005515) - optional column; only displayed if –goterms option is switched on)\n",
    "    15. (Pathways annotations (e.g. REACT_71)-optional column; only displayed if –pathways option is switched on)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5ad88",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "How many distinct protein annotations are found in the dataset? I.e. how many distinc InterPRO numbers are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "280685c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:=========================================>            (154 + 1) / 200]\r",
      "\r",
      "[Stage 21:=====================================================>(198 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "InterPRO_distinc = df.select(\"_c11\").distinct().count()\n",
    "print(InterPRO_distinc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b717d3a",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "How many annotations does a protein have on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d1d9848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_annot = df.select(\"_c11\").count()\n",
    "total_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31f5ee69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.278688524590164\n"
     ]
    }
   ],
   "source": [
    "avg_annot = total_annot/InterPRO_distinc\n",
    "print(avg_annot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddacefbe",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "What is the most common GO Term found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f345f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('state').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fbec694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupBy(\"_c13\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d509bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8a1545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn('_c13_GO1', split(df['_c13'], '-').getItem(0)) \\\n",
    "       .withColumn('_c13_GO2', split(df['_c13'], '-').getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b8fcc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "GO1 = df1.select('_c13_GO1')\n",
    "GO2 = df1.select('_c13_GO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69cc943e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(GO1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa4ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
